Programming Project 2 - Minix User Space Scheduler
Xiaoyuan Lu, Yongfeng Zhang
CMPS111, Winter 2014

====== Purpose (referenced from the specification pdf) ======

The main goal of this project is to modify the MINIX 3 user-space scheduler to be more flexible, by implementing a lottery scheduler.
We are supposed to know how to experiment with o23rat23g s21t14 kernels, and to do work in such a way that might crash a computer. It is
to provide us with experience examing the source code of an operating system and modifying the kernel to implement what we want. We are
also supposed to learn how to manage multiple kernels.

====== Available Resources ======

We are provided with the MINIX 3.1.8, which moved its scheduler into user space, and a report on the User Mode Scheduling in MINIX3, which
documents its kernel-space scheduling strategy (Round Robin), the processes' priority hierarchy and the message passing mechanism. We also
have access to some basic instructions on how to implement the lottery scheduling on a high level, given by the TA, such as how to spare
some queues in which the user processes are supposed to be run. Besides, since we are going to modify the kernel, we are instructed on how
to recompile the server modules and the kernel, and how to boot from different images we installed.

====== Design ======

First, we are only going to implement the lottery scheduling strategy on user processes and allocate tickets to them, since many system
tasks that start at priority 0 are directly initiated by the kernel, rather than by the do_start_scheduling function, making it hard to
track them and give them tickets.

Second, we are not going to add any more queues to the existing 16 queues. Originally, we did, but defining the NR_SCHED_QUEUES macro any
value more than 16 makes the system unable to boot in out virtual machine. The causes are still unclear. Therefore, we're going to use
queue 11 for loser processes, and queue 7 for the winner processes. Whenever a user process is started, we put it into the losers'
queue first. When a process wins the lottery, we move it to the winner queue and call schedule_process to schedule it. Every time the CPU
becomes free, we should play the lottery to choose the winner process and move it to the winner queue, so when there's no higher order
system tasks, this winner process will get the CPU. Therefore, whenever the do_noquantum() and do_stop_scheduling() get called, we need to
call our play_lottery() function to decide who will get the newly released CPU next.

Then I will illustrate what we added to the schedproc struct in the schedproc.h header file, since it reflects some top-level ideas about
the design. We added two data members to it, one is unsigned ticket_num which stores the number of tickets the process has, another is 
unsigned user_p, which is used to identify the process as either a user process or a system process.

We want to differentiate them mainly for two reasons:

The first one is that we want important system processes to be always under queue 7, which ensures that they'll be run in time whenever they
are ready. So in the do_noquantum() function, if the user_p denotes a process as a system process, then we only increase its priority when
it is smaller than or equal to 6. And in the balance_queues() function, we also only decrease system processes's priority periodically. We
don't decrease our user processes's priority because we want them to be fully under our control, which means that they can only switch 
between the losers' queue and the winners' queue. This is very important if you don't want the system to get stuck. We initially commented 
out the balance_queues() function totally to get a better control of the user processes, which, however, caused the system to get stuck at boot time.

The second reason is that in our play_lottery() function, we need to scan all of the user processes that are in use (denoted by IN_USE) and
collect their tickets and do a lottery among them, since we're only implementing this strategy on user processes. So here we're also going to
use that flag to filter out system processes.

So our struct is as follows:

EXTERN struct schedproc {
	endpoint_t endpoint;	/* process endpoint id */
	endpoint_t parent;	/* parent endpoint id */
	unsigned flags;		/* flag bits */

	/* User space scheduling */
	unsigned max_priority;	/* this process' highest allowed priority */
	unsigned priority;		/* the process' current priority */
	unsigned time_slice;		/* this process's time slice */
	unsigned ticket_num; 		/* number of tickets of the process */
	unsigned user_p;		/* to denote whether it's a system process or a user process */
} schedproc[NR_PROCS];

The way we initiate user_p is like this: under the case of SCHEDULING_INHERIT in the do_start_scheduling function, we assign it value 1,
and under the SCHEDULING_START case, we assign it value 2. The processes that are scheduled directly by the kernel (they have priority 0)
get the default value of unsigned, which is 0.

====== Functions Modified and Added, Basic Lottery Scheduling ======

First I'm going to describe how we implemented the basic lottery scheduling, once the structure is there, it's easy to understand task 2 and 3.

1. In do_noquantum() function:
	
	For the user processes, we first put it back to the losers' queue and then call play_lottery() function immediately to choose the next
	winner. For system processes whose priority is less than 7, we lower its priority by one, which is the original implementation that
	we want to keep unchanged.

2. We didn't change the do_stop_scheduling function.

3. In do_start_scheduling() function:

	Under the SCHEDULING_START case, we assign value 2 to the user_p to mark it as a system process.
	Under the SCHEDULING_INHERIT case, we assign 5 tickets and assign 1 to user_p to denote user process.
	We didn't change anything else in it.

4. In do_nice() function:
	
	We commented out all the instructions that can the process's priority, since in lottery scheduling, we only want to use the nice
	command to change the number of tickets of a process, rather than its priority. We assign tickets by directly passing the 		SCHEDULING_MAXPRIO attribute in the message pointer to our allot_ticket() function, which  will add the process "SCHEDULING_MAXPRIO" 		number of tickets. But here's a subtlty, we get to know that there is actually a mapping in the pm between the nice command argument
	and SCHEDULING_MAXPRIO. The mathematical relation is SCHEDULING_MAXPRIO = x/5 + USER_Q, where x is the argument we passed to nice.
	However, we didn't hack into the pm to make an identical mapping between the two variables since we don't have enough time left. But
	I think it's okay as long as we know how many tickets are assigned to each process after we run the nice command with a certain
	argument. Thus we can verify the correctness of our scheduling policy.

5. In balance_queues() function:

	We modified it so that we don't do anything for user processes, for system processes, we deduct its priority by one each time this
	function is called.

6. play_lottery() function:

	We added this function to collect the tickets of each user process and to choose the lottery winner. We used the first for loop to
	scan all the eligible user processes and sum their number of tickets. We then choose a lucky number randomly according to the number
	of tickets we collected. The winner will be put into the MAX_USER_Q queue and schedule_processed.

7. allot_ticket() function:

	This function is used to allocate tickets, it adds a certain number of tickets (denoted by the second argument) to a process if the
	result is within 100, otherwise, the process's ticket number will just be 100.


====== Lottery Scheduling With Dynamic Priorities ======

We further modified our do_noquantum() function and play_lottery() function to achieve it. Since our user-space scheduler won't get any
notification when a process blocks, in order to have the effect described in the specification, we used SCHEDULING_ACNT_IPC_SYNC attribute
of the message pointer to get the total number of blocks of a process when it runs out of quantum, and allocate all the tickets it should
get all at once in do_noquantum(), followed by a lottery draw. And we made sure that the total number of tickets of each process will not
exceed 5, which is the process's original number of tickets.

In play_lottery(), we added a couple of codes so that each time before we're going to schedule_process the lottery winner, we deduct one
ticket from it if it has more than one ticket.

====== Our Own Design ======






====== Tests ======

Basic Lottery:

We fire up two longrun processes (in longrun1.c), give them 12 and 20 tickets respectively, and redirect their outputs to the same file.
Once process prints "a" and the other prints "b" while running. Their total number of outputs are the same, but the latter process will end
earlier because it has more tickets. After both of them are done, we'll get a file containing a bunch of as and bs, and their countings.
So we can expect that after a certain line, there's going to be only as, because process b has already done. So by locating the last occurance
of b in the file (we backward searched b at the first line of the output file), and comparing the count of the last b and the count of its
next a, we can know the number of prints process a finished by the time process b is totally done. In our test, we have:

   .
   .
   .
b:299970     /* the last ocurrence of b */
a:206141
a:206242
a:206343
   .
   .
   .


So the CPU time of a to b is roughly 2:3, which is quite close to 12:20. Note that we modified the original longrun.c, so that it prints every
100 iteration counts, which makes a and b both start to count at 101, but since the total counts is much larger than 100, so the ratio of a:b
still reflects their CPU usage.

Scheduling with Dynamic Priorities:

The test for this one is much more straightforward, since lottery scheduling is already verified, we only need to show that the dynamic
allocation of tickets is in agree with the requirements.

First, to test whether at each block the process is allocated one more ticket, we added a printf in do_noquantum() that prints out the number
of blocks of a process by the time it runs out of quantum and its old and new ticket numbers. And in play_lottery() we added another printf
to check whether the process's ticket number will be deducted by one when it receives a full quantum. It's easy to see whether it's running
in a correct way by examing these values.






	
	






  
